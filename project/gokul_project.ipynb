{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Full Stack Machine Learning's Week 1 Project!\n",
    "\n",
    "Welcome to an exhilarating journey into the world of machine learning! This week, you're stepping into the shoes of a data scientist at ModaMetric, an eCommerce startup specializing in bespoke women's fashion. \n",
    "\n",
    "## The ModaMetric Journey\n",
    "\n",
    "ModaMetric is a budding star in the eCommerce landscape, gaining rapid popularity among customers for its unique and stylish fashion offerings. However, they've faced a bottleneck: understanding their customers' sentiments from the ocean of reviews and feedback they receive daily. Your role as a member of ModaMetric's data science team is pivotal in solving this issue. Your mission? To implement an effective sentiment analysis model that can sift through the plethora of customer reviews and provide actionable insights.\n",
    "\n",
    "## Your Role: A Pioneer Data Scientist at ModaMetric\n",
    "\n",
    "As ModaMetric's newly onboarded data scientist, you're entrusted with an exciting challenge. The data science team, still in its infancy, has primarily focused on metrics and analytics that provide surface-level insights. They've yet to delve into the rich, unstructured data residing in customer reviews. And that's where you step in. Your job is to design and implement a machine learning pipeline using Metaflow capable of performing sentiment analysis on the customer reviews.\n",
    " \n",
    "\n",
    "### Using GitHub\n",
    "\n",
    "To complete the assignment:\n",
    "1. Fill in the TODO sections of this notebook.\n",
    "2. Push the results to your `full-stack-ml-metaflow-corise-week-1` repository.\n",
    "3. Create a link to the repository in Corise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're starting with a [Women's Ecommerce Clothing Reviews Dataset from Kaggle](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews). This dataset closely mirrors the data that ModaMetric gathers. Your first task involves understanding the structure, variables, and potential quirks of this dataset. Remember, knowing your data is the first step in any data science project!\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical step in the data science pipeline as it allows us to gain insights and identify patterns within the data. In this section, we will be performing EDA on the Women's Clothing E-Commerce dataset, which contains reviews written by customers. Through this process, we will be looking out for trends, anomalies, and outliers that can help us better understand the data and inform our decision-making in subsequent stages of the project. By performing EDA, we will be able to identify potential issues with the dataset and make necessary corrections before proceeding to the model building phase.\n",
    "\n",
    "Suggestion: Spend 1-2 hours on this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies\n",
    "You can change these if you wish! \n",
    "These packages are already installed in the `full-stack-metaflow-corise` environment. \n",
    "If you are feeling adventurous, you can install other packages you want in the conda environment too, or even make your own environment from scratch and include with your submission! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure plots\n",
    "This part is optional styling your plots and cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YELLOW = \"#FFBC00\"\n",
    "GREEN = \"#37795D\"\n",
    "PURPLE = \"#5460C0\"\n",
    "BACKGROUND = \"#F4EBE6\"\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.facecolor\": BACKGROUND,\n",
    "    \"figure.facecolor\": BACKGROUND,\n",
    "    \"figure.figsize\": (8, 8),\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style=\"ticks\", rc=custom_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/Womens Clothing E-Commerce Reviews.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO: Load the dataset, ensure to use index_col=0 when reading the CSV file.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Hints\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Look in the ../../data directory of this worksapce.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# If you use pandas think about the index_col arg 🧐\u001b[39;00m\n\u001b[1;32m      5\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../../data/Womens Clothing E-Commerce Reviews.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# light data cleaning\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()) \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/mambaforge/envs/full-stack-metaflow-corise/lib/python3.10/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/Womens Clothing E-Commerce Reviews.csv'"
     ]
    }
   ],
   "source": [
    "# TODO: Load the dataset, ensure to use index_col=0 when reading the CSV file.\n",
    "# Hints\n",
    "# Look in the ../../data directory of this worksapce.\n",
    "# If you use pandas think about the index_col arg 🧐\n",
    "path = \"../../data/Womens Clothing E-Commerce Reviews.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "# light data cleaning\n",
    "df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "df[\"review_text\"] = df[\"review_text\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distribution of [1, 5] ratings\n",
    "\n",
    "We will be using the `rating` to create a label on this dataset. We can see that the mean rating is above 4, pretty happy customers!\n",
    "\n",
    "Let's try to visualise the distrbution of the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Derive the rating_distribution and plot it\n",
    "rating_distribution = ...\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "# You can swap the color used with the defined constants at the top of the notebook\n",
    "ax.bar(x=rating_distribution.index, height=rating_distribution.values, color=GREEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_function(row):\n",
    "    \"\"\"\n",
    "    A function to derive labels from the user's review data.\n",
    "    This could use many variables, or just one.\n",
    "    In supervised learning scenarios, this is a very important part of determining what the machine learns!\n",
    "\n",
    "    A subset of variables in the e-commerce fashion review dataset to consider for labels you could use in ML tasks include:\n",
    "        # rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "        # recommended_ind: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "        # positive_feedback_count: Positive Integer documenting the number of other customers who found this review positive.\n",
    "\n",
    "    In this case, we are doing sentiment analysis.\n",
    "    To keep things simple, we use the rating only, and return a binary positive or negative sentiment score based on an arbitrarty cutoff.\n",
    "    \"\"\"\n",
    "    # TODO: Add your logic for the labelling function here\n",
    "    # It is up to you on what value to choose as the cut off point for the postive class\n",
    "    # A good value to start would be 4\n",
    "    # This function should return either a 0 or 1 depending on the rating of a particular row\n",
    "    return\n",
    "\n",
    "\n",
    "# final features and labels\n",
    "_has_review_df = df[df[\"review_text\"] != \"nan\"]\n",
    "reviews = _has_review_df[\"review_text\"]\n",
    "labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "has_review_df = pd.DataFrame({\"label\": labels, **_has_review_df})\n",
    "del _has_review_df\n",
    "\n",
    "# a few checks\n",
    "assert (\n",
    "    labels.shape == reviews.shape\n",
    "), \"Labels and reviews should be equal shape vectors!\"\n",
    "assert (\n",
    "    not sum([1 if r == \"nan\" else 0 for r in reviews]) > 0\n",
    "), \"There are `nan` values in the feature set!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of points does your algorithm label with positive sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_positive_sentiment = labels.sum() / labels.shape[0]\n",
    "print(f\"{round(100*pct_positive_sentiment,3)}% of the labels have positive sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us try to visualise the data that we just labeled depending on the rating. \n",
    "\n",
    "In a real world project, iterating at this point is crucial. You need to look through the way your data is labeled, and ensure it is aligned with your intuitive understanding and objectives of the algorithm. There are also automated tools to aid your label cleaning operations, such as [Cleanlab](https://github.com/cleanlab/cleanlab).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "positive_color = \"green\"\n",
    "negative_color = \"red\"\n",
    "N = 10\n",
    "\n",
    "# fetch subset of data\n",
    "idxs = np.random.choice(reviews.index, 10, replace=False)\n",
    "_labels_subset = labels[idxs]\n",
    "_reviews_subset = reviews[idxs]\n",
    "\n",
    "# print each sample and color the text by sentiment\n",
    "for label, review in zip(_labels_subset, _reviews_subset):\n",
    "    color = negative_color if label == 0 else positive_color\n",
    "    print(colored(review, color), end=\"\\n\\n\")\n",
    "\n",
    "# in a real world project, iterating at this point is crucial.\n",
    "# you need to look through the way your data is labeled, and ensure it is aligned with your intuitive understanding and objectives of the algorithm.\n",
    "# there are also automated tools to aid your label cleaning operations, such as: https://github.com/cleanlab/cleanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think about the text and their corresponding labels? \n",
    "- Do you think the labels fit the text? \n",
    "- If not what do you think we can do to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Stop Words\n",
    "In this part we will be filtering the stop words from the reviews. We remove stopwords in NLP datasets because there are words that do not carry much meaning on their own, and their presence can add noise to the analysis. These words are common and frequently occurring words such as \"a\", \"an\", \"the\", \"of\", and \"and\". \n",
    "\n",
    "Removing stopwords can improve the accuracy and efficiency of natural language processing tasks, such as sentiment analysis or topic modeling, by reducing the dimensionality of the data and increasing the signal-to-noise ratio. By removing these uninformative words, the resulting dataset may contain more meaningful information that can be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "non_stopwords = []\n",
    "for review in reviews:\n",
    "    for word in review.split():\n",
    "        word = word.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        if word == \"\":\n",
    "            continue\n",
    "        if not word.lower() in stopwords:\n",
    "            non_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the K most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 25\n",
    "words, counts = zip(*Counter(non_stopwords).most_common(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
    "plt.xticks(rotation=55)\n",
    "ax.bar(x=words, height=counts, color=GREEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the other features in the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=has_review_df, hue=\"label\", corner=True, palette=sns_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Scoping Out a Machine Learning Project\n",
    "\n",
    "As ModaMetric's newly onboarded data scientist, you're entrusted with an exciting challenge. The data science team, still in its infancy, has primarily focused on metrics and analytics that provide surface-level insights. They've yet to delve into the rich, unstructured data residing in customer reviews. And that's where you step in. Your job is to design and implement a machine learning pipeline using Metaflow capable of performing sentiment analysis on the customer reviews.\n",
    "\n",
    "As a Data Scientist, you know that a successful project requires not only technical skills but also effective project management. In this task, you will take on the role of a Data Scientist tasked with leading the development of a sentiment analysis classifier. You will be responsible for planning and executing the project, ensuring that it aligns with business goals, stays within scope, and delivers value to stakeholders.\n",
    "\n",
    "To do this, you will create a one-page document that outlines the business value of the project, its scope, how to measure and monitor success, and when to quit. This task is designed to challenge you to think holistically about the project, and to consider not only the technical details but also the broader context in which the project is situated. Good luck!\n",
    "\n",
    "Suggestion: Spend 1-2 hours on this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the section below with your answers!\n",
    "\n",
    "### 1. The business value\n",
    "\n",
    "### 2. The scope\n",
    "\n",
    "### 3. How to measure success\n",
    "\n",
    "### 4. How to monitor success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job completing Task 2! \n",
    "\n",
    "Why do you think it is important to create a one-page document prior to beginning work on the project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Baseline Machine Learning Flow\n",
    "\n",
    "Finally, you'll develop a baseline model. This initial model, while simple, is crucial as it provides a benchmark for later, more complex models. You'll process the data, implement a basic machine learning algorithm, and evaluate its performance.\n",
    "\n",
    "Throughout this journey, you're not just a data scientist - you're a trailblazer helping ModaMetric navigate the seas of unstructured data. Ready to dive in? \n",
    "\n",
    "A basic baseline in a machine learning model is the simplest possible model that can be used to make predictions on the dataset. The basic baseline can be as simple as predicting the most frequent class for a classification problem or the mean value of the target variable for a regression problem. The purpose of establishing a baseline is to provide a benchmark for evaluating the performance of more complex models. A model that cannot outperform the basic baseline is considered to be useless and should not be used in practice.\n",
    "\n",
    "Here you will need to convert the code from above that was used to perform preprocessing and EDA on the dataset and create a Flow to run in order to train a baseline model. \n",
    "\n",
    "**NOTE:** It is important to realise that this is being run as a separate file and therefore re-using functions from above will not work. \n",
    "\n",
    "Suggestion: Spend 2-4 hours on this section. Rememeber that the more organized your earlier work is, the easier it is to write flows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile baseline_flow.py\n",
    "from metaflow import (\n",
    "    FlowSpec,\n",
    "    step,\n",
    "    Flow,\n",
    "    current,\n",
    "    Parameter,\n",
    "    IncludeFile,\n",
    "    card,\n",
    "    current,\n",
    ")\n",
    "from metaflow.cards import Table, Markdown, Artifact\n",
    "\n",
    "# TODO move your labeling function from earlier in the notebook here\n",
    "labeling_function = lambda row: 0\n",
    "\n",
    "\n",
    "class BaselineNLPFlow(FlowSpec):\n",
    "    # We can define input parameters to a Flow using Parameters\n",
    "    # More info can be found here https://docs.metaflow.org/metaflow/basics#how-to-define-parameters-for-flows\n",
    "    split_size = Parameter(\"split-sz\", default=0.2)\n",
    "    # In order to use a file as an input parameter for a particular Flow we can use IncludeFile\n",
    "    # More information can be found here https://docs.metaflow.org/api/flowspec#includefile\n",
    "    data = IncludeFile(\"data\", default=\"../data/Womens Clothing E-Commerce Reviews.csv\")\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        # Step-level dependencies are loaded within a Step, instead of loading them\n",
    "        # from the top of the file. This helps us isolate dependencies in a tight scope.\n",
    "        import pandas as pd\n",
    "        import io\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        # load dataset packaged with the flow.\n",
    "        # this technique is convenient when working with small datasets that need to move to remove tasks.\n",
    "        df = pd.read_csv(io.StringIO(self.data))\n",
    "\n",
    "        # filter down to reviews and labels\n",
    "        df.columns = [\"_\".join(name.lower().strip().split()) for name in df.columns]\n",
    "        df[\"review_text\"] = df[\"review_text\"].astype(\"str\")\n",
    "        _has_review_df = df[df[\"review_text\"] != \"nan\"]\n",
    "        reviews = _has_review_df[\"review_text\"]\n",
    "        labels = _has_review_df.apply(labeling_function, axis=1)\n",
    "        # Storing the Dataframe as an instance variable of the class\n",
    "        # allows us to share it across all Steps\n",
    "        # self.df is referred to as a Data Artifact now\n",
    "        # You can read more about it here https://docs.metaflow.org/metaflow/basics#artifacts\n",
    "        self.df = pd.DataFrame({\"label\": labels, **_has_review_df})\n",
    "        del df\n",
    "        del _has_review_df\n",
    "\n",
    "        # split the data 80/20, or by using the flow's split-sz CLI argument\n",
    "        _df = pd.DataFrame({\"review\": reviews, \"label\": labels})\n",
    "        self.traindf, self.valdf = train_test_split(_df, test_size=self.split_size)\n",
    "        print(f\"num of rows in train set: {self.traindf.shape[0]}\")\n",
    "        print(f\"num of rows in validation set: {self.valdf.shape[0]}\")\n",
    "\n",
    "        self.next(self.baseline)\n",
    "\n",
    "    @step\n",
    "    def baseline(self):\n",
    "        \"Compute the baseline\"\n",
    "\n",
    "        ### TODO: Fit and score a baseline model on the data, log the acc and rocauc as artifacts.\n",
    "        self.base_acc = 0.0\n",
    "        self.base_rocauc = 0.0\n",
    "\n",
    "        self.next(self.end)\n",
    "\n",
    "    @card(\n",
    "        type=\"corise\"\n",
    "    )  # TODO: after you get the flow working, chain link on the left side nav to open your card!\n",
    "    @step\n",
    "    def end(self):\n",
    "        msg = \"Baseline Accuracy: {}\\nBaseline AUC: {}\"\n",
    "        print(msg.format(round(self.base_acc, 3), round(self.base_rocauc, 3)))\n",
    "\n",
    "        current.card.append(Markdown(\"# Womens Clothing Review Results\"))\n",
    "        current.card.append(Markdown(\"## Overall Accuracy\"))\n",
    "        current.card.append(Artifact(self.base_acc))\n",
    "\n",
    "        current.card.append(Markdown(\"## Examples of False Positives\"))\n",
    "        # TODO: compute the false positive predictions where the baseline is 1 and the valdf label is 0.\n",
    "        # TODO: display the false_positives dataframe using metaflow.cards\n",
    "        # Documentation: https://docs.metaflow.org/api/cards#table\n",
    "\n",
    "        current.card.append(Markdown(\"## Examples of False Negatives\"))\n",
    "        # TODO: compute the false positive predictions where the baseline is 0 and the valdf label is 1.\n",
    "        # TODO: display the false_negatives dataframe using metaflow.cards\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BaselineNLPFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python baseline_flow.py run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job completing Task 3!\n",
    "\n",
    "The project for Week 1 is completed but you are free to try out Task 4 below if you have the time to do so! Remember that completing Task 4 is not a requirement and completely optional. So far we have got you already building basic Machine Learning Pipelines uing Metaflow, what do you think about it so far?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Venturing Beyond the Baseline - Creating a Good First Machine Learning Model (OPTIONAL)\n",
    "\n",
    "### The ModaMetric Challenge: Beyond the Baseline\n",
    "\n",
    "At ModaMetric, your initial explorations have been fruitful. You've established a baseline sentiment analysis model, and the team is abuzz with excitement. However, the baseline model is only the beginning. Your next mission is to create a more sophisticated model that outperforms the baseline.\n",
    "\n",
    "The eCommerce world moves at breakneck speed, and ModaMetric is no exception. Rapid iterations and quick feedback are the need of the hour. As you strive to improve the model, keep in mind the balance between complexity and practicality. More complex deep neural networks might offer higher accuracy, but they require GPUs for inference, which adds to the overhead. Simpler models, on the other hand, might have lower accuracy but can be deployed quickly and cost-effectively on CPUs.\n",
    "\n",
    "As you embark on this task, consider the following questions:\n",
    "\n",
    "1. Given the balance between accuracy and deployability, what model do you think is good enough to get the ball rolling at ModaMetric?\n",
    "2. Try to implement your choice in a new Metaflow Flow, `GoodFirstModelNLPFlow`.\n",
    "3. Can you confirm that `GoodFirstModelNLPFlow` performs better than `BaselineNLPFlow`? Metaflow's Client API can help you validate this. You can refer to the documentation [here](https://docs.metaflow.org/metaflow/client).\n",
    "4. Did `GoodFirstModelNLPFlow` outperform `BaselineNLPFlow`? If yes, why do you think it did? If not, what might be the reasons for not reaching the expected performance?\n",
    "5. Reflect on the importance of quick iterations in ML projects, especially in the context of ModaMetric's fast-paced environment.\n",
    "\n",
    "This optional task offers a chance to deepen your understanding of machine learning workflows, testing your ability to innovate and improvise in a real-world scenario. Remember, every step forward is a step towards ModaMetric's success!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
